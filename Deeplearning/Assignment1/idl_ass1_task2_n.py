# -*- coding: utf-8 -*-
"""idl-ass1-task2_n.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e07DSknUHG9hEYxM4GP2DULIr8zuu9fN

# Task 2--Tell The Time
"""

from __future__ import print_function
import keras
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Flatten, Input
from keras.layers import Conv2D, MaxPooling2D,BatchNormalization
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from keras import backend as K
import numpy as np
from keras.initializers import HeNormal, HeUniform
from keras.optimizers import SGD,Adam,RMSprop,Adagrad,Nadam
import matplotlib.pyplot as plt
from tensorflow.keras import regularizers

"""## Load data"""

x_data  = np.load("/kaggle/input/clock-large/images.npy")
y_data  = np.load("/kaggle/input/clock-large/labels.npy")
print(x_data.shape)

row_cnt=x_data.shape[1]
colomn_cnt=x_data.shape[2]
if K.image_data_format() == 'channel_first':
    x_data=x_data.reshape(x_data.shape[0],1,row_cnt,colomn_cnt)
    input_shape=(1,row_cnt,colomn_cnt)
else:
    x_data= x_data.reshape(x_data.shape[0],row_cnt,colomn_cnt,1)
    input_shape=(row_cnt,colomn_cnt,1)

print(input_shape,x_data.shape)

(x_train,x_test,y_train,y_test)= train_test_split(x_data,y_data,test_size=0.2)
(x_validate,x_train) = (x_train[:1800],x_train[1800:])
(y_validate,y_train) = (y_train[:1800],y_train[1800:])

print(x_train.shape,x_validate.shape,x_test.shape,y_train.shape,y_validate.shape,y_test.shape)

x_train = x_train.astype('float32')/255
x_test = x_test.astype('float32')/255
x_validate = x_validate.astype('float32')/255

"""## Classification Model

Dividing the total 720 minutes into n regions to treat the telling-clock problem as an n-class classification problem.

**Process labels**
"""

def processing_c_labels(categories):
    catergory_range= 720/categories
    y_train_class= ((y_train[:,0]*60+ y_train[:,1])/catergory_range).astype(np.int32)
    y_test_class= ((y_test[:,0]*60+ y_test[:,1])/catergory_range).astype(np.int32)
    y_validate_class= ((y_validate[:,0]*60+ y_validate[:,1])/catergory_range).astype(np.int32)
    return (y_train_class,y_test_class,y_validate_class)

"""**Build the model**"""

def build_model_task2_c1(categories,input_shape):
    inputs = Input(shape=input_shape)
    x = Conv2D(32,kernel_size=(7,7),activation='relu',padding='same',kernel_initializer='he_normal')(inputs)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=(2,2))(x)
    x= Dropout(0.2)(x)

    x = Conv2D(32,kernel_size=(5,5),activation='relu',padding='same',kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=(2,2))(x)
    x= Dropout(0.2)(x)

    x = Conv2D(64,kernel_size=(3,3),activation='relu', padding='same',kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=(2,2))(x)
    #x = Dropout(0.1)(x)

    x = Conv2D(64,kernel_size=(3,3),activation='relu', padding='same', dilation_rate=2, kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=(2,2))(x)
    # #x= Dropout(0.2)(x)

    x = Conv2D(128,kernel_size=(3,3),activation='relu', padding='same',kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=(2,2))(x)
    #x= Dropout(0.2)(x)

    x = Conv2D(128,kernel_size=(3,3),activation='relu', padding='same',kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=(2,2))(x)

    x= Flatten()(x)
    x= Dense(256, activation='relu', kernel_initializer='he_normal')(x)
    x= Dropout(0.4)(x)

    output= Dense(categories, activation='softmax')(x)
    model = Model(inputs=[inputs], outputs=[output])

    # Define the learning rate scheduler
    initial_learning_rate = 0.001
    lr_schedule = keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=initial_learning_rate,
        decay_steps=200,
        decay_rate=0.96,
        staircase=True
    )

    # Set up the optimizer with the learning rate schedule
    opt = keras.optimizers.Adam(learning_rate=lr_schedule)
    #opt = Adam(learning_rate=0.001)
    model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model

"""**Train & Evaluate the model**"""

def plot_training_history(history, accuracy_title='Model Accuracy', loss_title='Model Loss'):

    plt.figure(figsize=(8, 4))
    plt.subplot(1, 2, 1)

    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title(accuracy_title)
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend(loc='best')

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(loss_title)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend(loc='best')

    plt.tight_layout()
    plt.savefig(f'{accuracy_title}.png')
    plt.show()



def evaluate_time_error_metrics(y_test_class, y_pred_class, classnum, threshold):
    # category_range = 720 / classnum
    # y_test_time = y_test_class * category_range
    # y_pred_time = y_pred_class * category_range
    # time_diff = np.abs(y_pred_time - y_test_time)
    # time_diff = np.minimum(time_diff, 720 - time_diff)
    time_diff = np.abs(y_pred_class - y_test_class)
    time_diff = np.minimum(time_diff, classnum - time_diff)

    # Calculate metrics
    mean_error = np.mean(time_diff)
    max_time_diff = np.max(time_diff)
    median_error = np.median(time_diff)
    std_dev_error = np.std(time_diff)
    percentage_within_threshold = np.mean(time_diff <= threshold) * 100
    predict_accuracy = np.mean(time_diff <= 0) * 100

    error_metrics = {
        'Mean Error (Time Zone)': mean_error,
        'Max Time Difference (Time Zone)': max_time_diff,
        'Median Error (Time Zone)': median_error,
        'Standard Deviation of Error (Time Zone)': std_dev_error,
        'Predict_accuracy(%)':predict_accuracy,
        f'Percentage within {threshold} Time Zone': percentage_within_threshold
    }

    return error_metrics

"""**Try 24 categories**"""

classnum=24
(y_train_class,y_test_class,y_validate_class) = processing_c_labels(classnum)
model_c1 = build_model_task2_c1(classnum,input_shape)

#from tensorflow.keras.utils import plot_model
#plot_model(model_c1, to_file='model_cl.png', show_shapes=True, show_layer_names=True)

early_stopping_cb = keras.callbacks.EarlyStopping(patience=20,restore_best_weights=True)
history_c1 = model_c1.fit(x_train,y_train_class,epochs=100,batch_size=32,verbose=1,validation_data=(x_validate,y_validate_class),callbacks=[early_stopping_cb])
plot_training_history(history_c1,f'model accuray -{classnum} categories',f'model loss -{classnum} categories')

#keras.utils.plot_model(model_c1, to_file='model.png', show_shapes=True)
classnum=24
loss, accuracy = model_c1.evaluate(x_test, y_test_class)
print(f"Test loss: {loss}")
print(f"Test accuracy: {accuracy}")

y_pred_class = np.argmax(model_c1.predict(x_test), axis=1)
error_metrics = evaluate_time_error_metrics(y_test_class, y_pred_class, classnum, threshold=1)

# Print results
for metric, value in error_metrics.items():
    print(f"{metric}: {value}")

"""**Try 36 categories**"""

classnum=36
(y_train_class,y_test_class,y_validate_class) = processing_c_labels(classnum)
model_c2 = build_model_task2_c1(classnum,input_shape)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=20,restore_best_weights=True)
history_c2 = model_c2.fit(x_train,y_train_class,epochs=100,batch_size=32,verbose=1,validation_data=(x_validate,y_validate_class),callbacks=[early_stopping_cb])
plot_training_history(history_c2,f'model accuray -{classnum} categories',f'model loss -{classnum} categories')

loss, accuracy = model_c2.evaluate(x_test, y_test_class)
print(f"Test loss: {loss}")
print(f"Test accuracy: {accuracy}")
y_pred_class = np.argmax(model_c2.predict(x_test), axis=1)
error_metrics = evaluate_time_error_metrics(y_test_class, y_pred_class, classnum, threshold=1)

# Print results
for metric, value in error_metrics.items():
    print(f"{metric}: {value}")

"""**Try 48 categories**"""

classnum=48

(y_train_class,y_test_class,y_validate_class) = processing_c_labels(classnum)
model_c3 = build_model_task2_c1(classnum,input_shape)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=20,restore_best_weights=True)
history_c3 = model_c3.fit(x_train,y_train_class,epochs=100,batch_size=32,verbose=1,validation_data=(x_validate,y_validate_class),callbacks=[early_stopping_cb])
plot_training_history(history_c3,f'model accuray -{classnum} categories',f'model loss -{classnum} categories')

loss, accuracy = model_c3.evaluate(x_test, y_test_class)
print(f"Test loss: {loss}")
print(f"Test accuracy: {accuracy}")

y_pred_class = np.argmax(model_c3.predict(x_test), axis=1)
error_metrics = evaluate_time_error_metrics(y_test_class, y_pred_class, classnum, threshold=1)

# Print results
for metric, value in error_metrics.items():
    print(f"{metric}: {value}")

"""**Try 72 categories**"""

classnum=72
(y_train_class,y_test_class,y_validate_class) = processing_c_labels(classnum)
model_c4 = build_model_task2_c1(classnum,input_shape)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=20,restore_best_weights=True)
history_c4 = model_c4.fit(x_train,y_train_class,epochs=100,batch_size=32,verbose=1,validation_data=(x_validate,y_validate_class),callbacks=[early_stopping_cb])
plot_training_history(history_c4,f'model accuray -{classnum} categories',f'model loss -{classnum} categories')

loss, accuracy = model_c4.evaluate(x_test, y_test_class)
print(f"Test loss: {loss}")
print(f"Test accuracy: {accuracy}")

y_pred_class = np.argmax(model_c4.predict(x_test), axis=1)
error_metrics = evaluate_time_error_metrics(y_test_class, y_pred_class, classnum, threshold=1)

# Print results
for metric, value in error_metrics.items():
    print(f"{metric}: {value}")

"""**Try 120 categories**"""

classnum=120
(y_train_class,y_test_class,y_validate_class) = processing_c_labels(classnum)

model_c5 = build_model_task2_c1(classnum,input_shape)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=20,restore_best_weights=True)
history_c5 = model_c5.fit(x_train,y_train_class,epochs=100,batch_size=32,verbose=1,validation_data=(x_validate,y_validate_class),callbacks=[early_stopping_cb])
plot_training_history(history_c5,f'model accuray -{classnum} categories',f'model loss -{classnum} categories')

classnum=120
loss, accuracy = model_c5.evaluate(x_test, y_test_class)
print(f"Test loss: {loss}")
print(f"Test accuracy: {accuracy}")

y_pred_class = np.argmax(model_c5.predict(x_test), axis=1)
error_metrics = evaluate_time_error_metrics(y_test_class, y_pred_class, classnum, threshold=1)
# Print results
for metric, value in error_metrics.items():
    print(f"{metric}: {value}")

"""## Regression Model

**Process labels**
"""

y_train_sreg= (y_train[:,0] + y_train[:,1]/60).astype('float32')
y_test_sreg=  (y_test[:,0] + y_test[:,1]/60).astype('float32')
y_validate_sreg=  (y_validate[:,0] + y_validate[:,1]/60).astype('float32')

"""**Build the regression model**|"""

def build_model_task2_r1(input_shape):
    inputs = Input(shape=input_shape)
    x = Conv2D(32,kernel_size=(7,7),activation='relu', padding='same',kernel_initializer='he_normal')(inputs)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=(2,2))(x)

    x = Conv2D(32,kernel_size=(5,5),activation='relu', padding='same',kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=(2,2))(x)

    x = Conv2D(64,kernel_size=(3,3),activation='relu', padding='same',kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=(2,2))(x)


    x = Conv2D(128,kernel_size=(3,3),activation='relu', padding='same', dilation_rate=2, kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=(2,2))(x)

    x = Conv2D(128,kernel_size=(3,3),activation='relu', padding='same',kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=(2,2))(x)

    x = Conv2D(256,kernel_size=(3,3),activation='relu', padding='same',kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=(2,2))(x)

    x= Flatten()(x)
    x= Dense(256, activation='relu', kernel_initializer='he_normal')(x)
    x= Dropout(0.2)(x)

    output=Dense(1, activation='linear')(x)
    model = Model(inputs=[inputs], outputs=[output])

    initial_learning_rate = 0.001
    lr_schedule = keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=initial_learning_rate,
        decay_steps=200,
        decay_rate=0.96,
        staircase=True
    )
    opt = keras.optimizers.Adam(learning_rate=lr_schedule)

    model.compile(loss='mse', optimizer=opt, metrics=['mae'])
    return model

"""**Train the model**"""

def plot_regression_loss(history, loss_title='Regression Model Loss'):
    plt.figure(figsize=(8, 5))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(loss_title)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend(loc='best')
    plt.savefig("regression model loss.png")
    plt.show()



def plot_predicted_vs_true(y_true, y_pred):
    plt.figure(figsize=(6, 6))
    plt.scatter(y_true, y_pred, alpha=0.5)
    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')  # Line y=x for reference
    plt.xlabel('True Values')
    plt.ylabel('Predicted Values')
    plt.title('Predicted vs True Values in Regression Model')
    plt.savefig("true vs predict-regression.png")
    plt.show()

model_r = build_model_task2_r1(input_shape)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)
history_r = model_r.fit(x_train,y_train_sreg,epochs=100,batch_size=32,verbose=1,validation_data=(x_validate,y_validate_sreg),callbacks=[early_stopping_cb])

plot_regression_loss(history_r)

def calculate_common_sense_error_r(y_true, y_pred, threshold=10):
    y_true_minutes = (y_true * 60).astype(np.int32)
    y_pred_minutes = (y_pred * 60).astype(np.int32)

    time_diff = np.abs(y_pred_minutes - y_true_minutes)
    time_diff = np.minimum(time_diff, 720 - time_diff)

    mean_error = np.mean(time_diff)
    max_error = np.max(time_diff)
    median_error = np.median(time_diff)
    std_dev_error = np.std(time_diff)

    percentage_within_threshold = np.mean(time_diff <= threshold) * 100
    predict_accuracy =  np.mean(time_diff <= 0) * 100
    error_metrics = {
        'Mean Common Sense Error (minutes)': mean_error,
        'Max Common Sense Error (minutes)': max_error,
        'Median Common Sense Error (minutes)': median_error,
        'Standard Deviation of Error (minutes)': std_dev_error,
        'Predict Accuracy (%)': predict_accuracy,
        f'Percentage within {threshold} minutes': percentage_within_threshold
    }

    return error_metrics

y_predict = model_r.predict(x_test)
plot_predicted_vs_true(y_test_sreg,y_predict)

error_metrics = calculate_common_sense_error_r(y_test_sreg.reshape(-1,1), y_predict,10)
for metric, value in error_metrics.items():
    print(f"{metric}: {value}")

"""## Multi-head Model

**Process labels**
"""

y_train_mreg= (y_train[:,1]/60).astype('float32')
y_test_mreg=  (y_test[:,1]/60).astype('float32')
y_validate_mreg=  (y_validate[:,1]/60).astype('float32')

y_train_mclass= y_train[:,0]
y_test_mclass= y_test[:,0]
y_validate_mclass= y_validate[:,0]

"""**Build the multi head model**"""

def build_model_task2_rc(input_shape):
    inputs = Input(shape=input_shape)
    x = Conv2D(32,kernel_size=(7,7),activation='relu', padding='same',kernel_initializer='he_normal')(inputs)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=(2,2))(x)

    x = Conv2D(32,kernel_size=(5,5),activation='relu', padding='same',kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=(2,2))(x)

    x = Conv2D(64,kernel_size=(3,3),activation='relu', padding='same',kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=(2,2))(x)

    x = Conv2D(128,kernel_size=(3,3),activation='relu', padding='same', dilation_rate=2, kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=(2,2))(x)

    x = Conv2D(128,kernel_size=(3,3),activation='relu', padding='same',kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=(2,2))(x)

    x = Conv2D(256,kernel_size=(3,3),activation='relu', padding='same',kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=(2,2))(x)

    x= Flatten()(x)
    x= Dense(1024, activation='relu', kernel_initializer='he_normal')(x)
    x= Dropout(0.5)(x)


    hour_output= Dense(12, activation='softmax', name='hour_output')(x)
    minute_output = Dense(1, activation='linear', name='minute_output')(x)

    model = Model(inputs=[inputs],outputs=[hour_output,minute_output])

    initial_learning_rate = 0.001
    lr_schedule = keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=initial_learning_rate,
        decay_steps=200,
        decay_rate=0.96,
        staircase=True
    )
    opt = keras.optimizers.Adam(learning_rate=lr_schedule)

    loss_weights = {'hour_output': 10.0, 'minute_output': 1.0}
    model.compile(loss={'hour_output': 'sparse_categorical_crossentropy', 'minute_output': 'mse'},  loss_weights=loss_weights,
              optimizer=opt, metrics={'hour_output': 'accuracy', 'minute_output': 'mae'})

    return model

"""**Train the model**"""

def plot_multi_output_history(history):
    plt.figure(figsize=(15, 5))

    # Classification Accuracy for hour_output
    plt.subplot(1, 3, 1)
    plt.plot(history.history['hour_output_accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_hour_output_accuracy'], label='Validation Accuracy')
    plt.title('Hour Prediction Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend(loc='best')

    # Combined Loss (for both hour and minute outputs)
    plt.subplot(1, 3, 2)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Overall Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend(loc='best')


    # Regression Loss for minute_output (MAE)
    plt.subplot(1, 3, 3)
    plt.plot(history.history['minute_output_mae'], label='Training MAE')
    plt.plot(history.history['val_minute_output_mae'], label='Validation MAE')
    plt.title('Minute Prediction Error (MAE)')
    plt.xlabel('Epoch')
    plt.ylabel('Mean Absolute Error (MAE)')
    plt.legend(loc='best')

    plt.tight_layout()
    plt.savefig("multi_model_loss.png")
    plt.show()

model_rc = build_model_task2_rc(input_shape)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=20,restore_best_weights=True)

history_rc = model_rc.fit(x_train,[y_train_mclass,y_train_mreg],epochs=200,verbose=1,validation_data=(x_validate,[y_validate_mclass,y_validate_mreg]),callbacks=[#checkpoint_cb,
    early_stopping_cb])

plot_multi_output_history(history_rc)

"""**Evaluate the model**"""

loss, hour_accuracy, minute_mae = model_rc.evaluate(x_test, [y_test_mclass, y_test_mreg])
print(f"Test Loss (MSE): {loss}")
print(f"Test HOUR ACCURACY: {hour_accuracy}")
print(f"Test MINITE MAE: {minute_mae}")

def calculate_common_sense_error_rc(y_true_hours, y_true_minutes, y_pred_hours, y_pred_minutes, threshold=10):
    y_true_total_minutes = (y_true_hours * 60 + y_true_minutes).astype(np.int32)
    y_pred_total_minutes = (y_pred_hours * 60 + y_pred_minutes).astype(np.int32)

    time_diff = np.abs(y_pred_total_minutes - y_true_total_minutes)
    time_diff = np.minimum(time_diff, 720 - time_diff)

    mean_error = np.mean(time_diff)
    max_error = np.max(time_diff)
    median_error = np.median(time_diff)
    std_dev_error = np.std(time_diff)
    percentage_within_threshold = np.mean(time_diff <= threshold) * 100

    error_metrics = {
        'Mean Common Sense Error (minutes)': mean_error,
        'Max Common Sense Error (minutes)': max_error,
        'Median Common Sense Error (minutes)': median_error,
        'Standard Deviation of Error (minutes)': std_dev_error,
        f'Percentage within {threshold} minutes': percentage_within_threshold
    }

    return error_metrics

predictions = model_rc.predict(x_test)
y_pred_hours = np.argmax(predictions[0], axis=1)
y_pred_minutes = predictions[1]*60

error_metrics = calculate_common_sense_error_rc(y_test_mclass, y_test_mreg *60, y_pred_hours, y_pred_minutes, threshold=10)

for metric, value in error_metrics.items():
    print(f"{metric}: {value}")

"""## Label transformation - Multi-regression Model

**Process labels**
"""

def hands_angle(hour, minute):
    minute_angle = minute * 6
    hour_angle = (hour % 12) * 30 + (minute * 0.5)
    return (hour_angle, minute_angle)


vectorized_hands_angle = np.vectorize(hands_angle)

# Calculate hour and minute angles for training set
(y_train_hour_angle, y_train_minute_angle) = vectorized_hands_angle(y_train[:, 0], y_train[:, 1])
y_train_hour_x = np.cos(np.radians(y_train_hour_angle))
y_train_hour_y = np.sin(np.radians(y_train_hour_angle))
y_train_minute_x = np.cos(np.radians(y_train_minute_angle))
y_train_minute_y = np.sin(np.radians(y_train_minute_angle))


# Calculate hour and minute angles for test set
(y_test_hour_angle, y_test_minute_angle) = vectorized_hands_angle(y_test[:, 0], y_test[:, 1])
y_test_hour_x = np.cos(np.radians(y_test_hour_angle))
y_test_hour_y = np.sin(np.radians(y_test_hour_angle))
y_test_minute_x = np.cos(np.radians(y_test_minute_angle))
y_test_minute_y = np.sin(np.radians(y_test_minute_angle))


# Calculate hour and minute angles for validation set
(y_validate_hour_angle, y_validate_minute_angle) = vectorized_hands_angle(y_validate[:, 0], y_validate[:, 1])
y_validate_hour_x = np.cos(np.radians(y_validate_hour_angle))
y_validate_hour_y = np.sin(np.radians(y_validate_hour_angle))
y_validate_minute_x = np.cos(np.radians(y_validate_minute_angle))
y_validate_minute_y = np.sin(np.radians(y_validate_minute_angle))

"""**Build the multi regression model**"""

def build_model_task2_mr(input_shape):
    inputs = Input(shape=input_shape)
    x = Conv2D(32,kernel_size=(7,7),activation='relu', padding='same',kernel_initializer='he_normal')(inputs)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=(2,2))(x)

    x = Conv2D(32,kernel_size=(5,5),activation='relu', padding='same',kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=(2,2))(x)

    x = Conv2D(64,kernel_size=(3,3),activation='relu', padding='same',kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=(2,2))(x)

    x = Conv2D(128,kernel_size=(3,3),activation='relu', padding='same', dilation_rate=2, kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=(2,2))(x)

    x = Conv2D(128,kernel_size=(3,3),activation='relu', padding='same',kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=(2,2))(x)

    x = Conv2D(256,kernel_size=(3,3),activation='relu', padding='same',kernel_initializer='he_normal')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D(pool_size=(2,2))(x)

    x= Flatten()(x)
    x= Dense(1024, activation='relu', kernel_initializer='he_normal')(x)
    x= Dropout(0.1)(x)

    hour_output_x = Dense(1, activation='linear', name='hour_output_x')(x)
    hour_output_y = Dense(1, activation='linear', name='hour_output_y')(x)
    minute_output_x = Dense(1, activation='linear', name='minute_output_x')(x)
    minute_output_y = Dense(1, activation='linear', name='minute_output_y')(x)

    model = Model(inputs=[inputs], outputs=[hour_output_x, hour_output_y, minute_output_x, minute_output_y])

    initial_learning_rate = 0.001
    lr_schedule = keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=initial_learning_rate,
        decay_steps=400,
        decay_rate=0.96,
        staircase=True
    )

    opt = keras.optimizers.Adam(learning_rate=lr_schedule)

    model.compile(
        optimizer=opt,
        loss={
            'hour_output_x': 'mse',
            'hour_output_y': 'mse',
            'minute_output_x': 'mse',
            'minute_output_y': 'mse'
        },

        metrics={
            'hour_output_x': 'mae',
            'hour_output_y': 'mae',
            'minute_output_x': 'mae',
            'minute_output_y': 'mae'
        }
    )

    return model

"""**Train the model**"""

model_mr = build_model_task2_mr(input_shape)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=15,restore_best_weights=True)

history_rr = model_mr.fit(
     x_train,
    [y_train_hour_x, y_train_hour_y, y_train_minute_x, y_train_minute_y],
    epochs=200,
    verbose=1,
    validation_data=(
        x_validate,
        [y_validate_hour_x, y_validate_hour_y, y_validate_minute_x, y_validate_minute_y]
    ),
    callbacks=[early_stopping_cb]
)

def plot_multi_output_regression_history(history):
    plt.figure(figsize=(10, 3))

    # # Loss
    # plt.subplot(2, 3, 1)
    # plt.plot(history.history['loss'], label='Training Loss')
    # plt.plot(history.history['val_loss'], label='Validation Loss')
    # plt.title('Total Loss (MSE)')
    # plt.xlabel('Epoch')
    # plt.ylabel('Loss')
    # plt.legend(loc='best')

    # Hour Output X MAE
    plt.subplot(1, 4, 1)
    plt.plot(history.history['hour_output_x_mae'], label='Training MAE')
    plt.plot(history.history['val_hour_output_x_mae'], label='Validation MAE')
    plt.title('Hour Output X MAE')
    plt.xlabel('Epoch')
    plt.ylabel('MAE')
    plt.legend(loc='best')

    # Hour Output Y MAE
    plt.subplot(1, 4, 2)
    plt.plot(history.history['hour_output_y_mae'], label='Training MAE')
    plt.plot(history.history['val_hour_output_y_mae'], label='Validation MAE')
    plt.title('Hour Output Y MAE')
    plt.xlabel('Epoch')
    plt.ylabel('MAE')
    plt.legend(loc='best')

    # Minute Output X MAE
    plt.subplot(1,4, 3)
    plt.plot(history.history['minute_output_x_mae'], label='Training MAE')
    plt.plot(history.history['val_minute_output_x_mae'], label='Validation MAE')
    plt.title('Minute Output X MAE')
    plt.xlabel('Epoch')
    plt.ylabel('MAE')
    plt.legend(loc='best')


    # Minute Output Y MAE
    plt.subplot(1, 4, 4)
    plt.plot(history.history['minute_output_y_mae'], label='Training MAE')
    plt.plot(history.history['val_minute_output_y_mae'], label='Validation MAE')
    plt.title('Minute Output Y MAE')
    plt.xlabel('Epoch')
    plt.ylabel('MAE')
    plt.legend(loc='best')

    # Adjust layout
    plt.tight_layout()
    plt.savefig("multi_regression_loss.png")
    plt.show()


plot_multi_output_regression_history(history_rr)

"""Evaluate the model"""

def calculate_common_sense_error_rr(true_hours, true_minutes,
                                    pred_hour_x, pred_hour_y,
                                    pred_minute_x, pred_minute_y,
                                    threshold=10):

    pred_hour_angle = np.degrees(np.arctan2(pred_hour_y, pred_hour_x)) % 360
    pred_minute_angle = np.degrees(np.arctan2(pred_minute_y, pred_minute_x)) % 360

    pred_hours = ((pred_hour_angle / 30) % 12).astype(int)
    pred_minutes =((pred_minute_angle / 6)).astype(int)

    true_times_in_minutes = (true_hours % 12) * 60 + true_minutes
    pred_times_in_minutes = (pred_hours % 12) * 60 + pred_minutes

    time_diffs = np.abs(pred_times_in_minutes - true_times_in_minutes)
    time_diffs = np.minimum(time_diffs, 720 - time_diffs)


    # Calculate error metrics
    mean_error = np.mean(time_diffs)
    max_error = np.max(time_diffs)
    median_error = np.median(time_diffs)
    std_dev_error = np.std(time_diffs)
    percentage_within_threshold = np.mean(time_diffs <= threshold) * 100
    percentage_within_5min = np.mean(time_diffs <= 5) * 100

    error_metrics = {
        'Mean Total Minutes Error': mean_error,
        'Max Total Minutes Error': max_error,
        'Median Total Minutes Error': median_error,
        'Standard Deviation of Total Minutes Error': std_dev_error,
        #'percentage_within_5 min': percentage_within_5min,
        f'Percentage within {threshold} minutes': percentage_within_threshold
    }

    return error_metrics

true_hours = y_test[:, 0]
true_minutes = y_test[:, 1]

y_pred_hour_x, y_pred_hour_y, y_pred_minute_x, y_pred_minute_y = model_mr.predict(x_test)

error_metrics = calculate_common_sense_error_rr(true_hours.reshape(-1,1),true_minutes.reshape(-1,1),
    y_pred_hour_x, y_pred_hour_y, y_pred_minute_x, y_pred_minute_y,10)


# Print results
for metric, value in error_metrics.items():
    print(f"{metric}: {value}")

loss, hour_x,hour_y, minute_x, minute_y = model_mr.evaluate(x_test, [y_test_hour_x, y_test_hour_y, y_test_minute_x, y_test_minute_y])

print(f"Test Loss (MSE): {loss}")
print(f"Test HOUR X: {hour_x} Y:{hour_y}")
print(f"Test MINUTE X: {minute_x} Y:{minute_y}")

