{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"collapsed_sections":["FZ7Xx__nJmj7"],"provenance":[],"gpuType":"T4"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [markdown] {\"id\":\"FZ7Xx__nJmj7\"}\n# <div style=\"text-align: right\">   </div>\n# \n# \n# Introduction to Deep Learning (2024) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;\n# -------|-------------------\n# **Assignment 2 - Sequence processing using RNNs** | <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/UniversiteitLeidenLogo.svg/1280px-UniversiteitLeidenLogo.svg.png\" width=\"300\">\n# \n# \n# \n# # Introduction\n# \n# \n# The goal of this assignment is to learn how to use encoder-decoder recurrent neural networks (RNNs). Specifically we will be dealing with a sequence to sequence problem and try to build recurrent models that can learn the principles behind simple arithmetic operations (**integer addition, subtraction and multiplication.**).\n# \n# <img src=\"https://i.ibb.co/5Ky5pbk/Screenshot-2023-11-10-at-07-51-21.png\" alt=\"Screenshot-2023-11-10-at-07-51-21\" border=\"0\" width=\"500\"></a>\n# \n# In this assignment you will be working with three different kinds of models, based on input/output data modalities:\n# 1. **Text-to-text**: given a text query containing two integers and an operand between them (+ or -) the model's output should be a sequence of integers that match the actual arithmetic result of this operation\n# 2. **Image-to-text**: same as above, except the query is specified as a sequence of images containing individual digits and an operand.\n# 3. **Text-to-image**: the query is specified in text format as in the text-to-text model, however the model's output should be a sequence of images corresponding to the correct result.\n# \n# \n# ### Description**\n# Let us suppose that we want to develop a neural network that learns how to add or subtract\n# two integers that are at most two digits long. For example, given input strings of 5 characters: ‘81+24’ or\n# ’41-89’ that consist of 2 two-digit long integers and an operand between them, the network should return a\n# sequence of 3 characters: ‘105 ’ or ’-48 ’ that represent the result of their respective queries. Additionally,\n# we want to build a model that generalizes well - if the network can extract the underlying principles behind\n# the ’+’ and ’-’ operands and associated operations, it should not need too many training examples to generate\n# valid answers to unseen queries. To represent such queries we need 13 unique characters: 10 for digits (0-9),\n# 2 for the ’+’ and ’-’ operands and one for whitespaces ’ ’ used as padding.\n# The example above describes a text-to-text sequence mapping scenario. However, we can also use different\n# modalities of data to represent our queries or answers. For that purpose, the MNIST handwritten digit\n# dataset is going to be used again, however in a slightly different format. The functions below will be used to create our datasets.\n# \n# ---\n# \n# *To work on this notebook you should create a copy of it.*\n# \n\n# %% [markdown] {\"id\":\"sP__1utGJmj_\"}\n# # Function definitions for creating the datasets\n# \n# First we need to create our datasets that are going to be used for training our models.\n# \n# In order to create image queries of simple arithmetic operations such as '15+13' or '42-10' we need to create images of '+' and '-' signs using ***open-cv*** library. We will use these operand signs together with the MNIST dataset to represent the digits.\n\n# %% [code] {\"id\":\"w3nzYbXOJmj8\",\"execution\":{\"iopub.status.busy\":\"2024-12-05T14:42:51.491975Z\",\"iopub.execute_input\":\"2024-12-05T14:42:51.49236Z\",\"iopub.status.idle\":\"2024-12-05T14:42:51.497471Z\",\"shell.execute_reply.started\":\"2024-12-05T14:42:51.492331Z\",\"shell.execute_reply\":\"2024-12-05T14:42:51.496607Z\"}}\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport random\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.layers import Dense, RNN, LSTM, Flatten, TimeDistributed, LSTMCell\nfrom tensorflow.keras.layers import RepeatVector, Conv2D, SimpleRNN, GRU, Reshape, ConvLSTM2D, Conv2DTranspose\n\n# %% [code] {\"id\":\"Z7RGPMxLJmkA\",\"outputId\":\"ee7a921b-1829-44a2-cfc9-f2cb60bf92d5\",\"execution\":{\"iopub.status.busy\":\"2024-12-05T14:42:51.498965Z\",\"iopub.execute_input\":\"2024-12-05T14:42:51.49922Z\",\"iopub.status.idle\":\"2024-12-05T14:42:52.582932Z\",\"shell.execute_reply.started\":\"2024-12-05T14:42:51.499197Z\",\"shell.execute_reply\":\"2024-12-05T14:42:52.581405Z\"}}\nfrom scipy.ndimage import rotate\n\n\n# Create plus/minus operand signs\ndef generate_images(number_of_images=50, sign='-'):\n    blank_images = np.zeros([number_of_images, 28, 28])  # Dimensionality matches the size of MNIST images (28x28)\n    x = np.random.randint(12, 16, (number_of_images, 2)) # Randomized x coordinates\n    y1 = np.random.randint(6, 10, number_of_images)       # Randomized y coordinates\n    y2 = np.random.randint(18, 22, number_of_images)     # -||-\n\n    for i in range(number_of_images): # Generate n different images\n        cv2.line(blank_images[i], (y1[i], x[i,0]), (y2[i], x[i, 1]), (255,0,0), 2, cv2.LINE_AA)     # Draw lines with randomized coordinates\n        if sign == '+':\n            cv2.line(blank_images[i], (x[i,0], y1[i]), (x[i, 1], y2[i]), (255,0,0), 2, cv2.LINE_AA) # Draw lines with randomized coordinates\n        if sign == '*':\n            cv2.line(blank_images[i], (x[i,0], y1[i]), (x[i, 1], y2[i]), (255,0,0), 2, cv2.LINE_AA)\n            # Rotate 45 degrees\n            blank_images[i] = rotate(blank_images[i], -50, reshape=False)\n            cv2.line(blank_images[i], (x[i,0], y1[i]), (x[i, 1], y2[i]), (255,0,0), 2, cv2.LINE_AA)\n            blank_images[i] = rotate(blank_images[i], -50, reshape=False)\n            cv2.line(blank_images[i], (x[i,0], y1[i]), (x[i, 1], y2[i]), (255,0,0), 2, cv2.LINE_AA)\n\n    return blank_images\n\ndef show_generated(images, n=5):\n    plt.figure(figsize=(2, 2))\n    for i in range(n**2):\n        plt.subplot(n, n, i+1)\n        plt.axis('off')\n        plt.imshow(images[i])\n    plt.show()\n\nshow_generated(generate_images())\nshow_generated(generate_images(sign='+'))\n\n# %% [code] {\"id\":\"Lx-Pt6ypJmkD\",\"execution\":{\"iopub.status.busy\":\"2024-12-05T14:42:52.5849Z\",\"iopub.execute_input\":\"2024-12-05T14:42:52.585401Z\",\"iopub.status.idle\":\"2024-12-05T14:42:52.610147Z\",\"shell.execute_reply.started\":\"2024-12-05T14:42:52.585346Z\",\"shell.execute_reply\":\"2024-12-05T14:42:52.608971Z\"}}\ndef create_data(highest_integer, num_addends=2, operands=['+', '-']):\n    \"\"\"\n    Creates the following data for all pairs of integers up to [1:highest integer][+/-][1:highest_integer]:\n\n    @return:\n    X_text: '51+21' -> text query of an arithmetic operation (5)\n    X_img : Stack of MNIST images corresponding to the query (5 x 28 x 28) -> sequence of 5 images of size 28x28\n    y_text: '72' -> answer of the arithmetic text query\n    y_img :  Stack of MNIST images corresponding to the answer (3 x 28 x 28)\n\n    Images for digits are picked randomly from the whole MNIST dataset.\n    \"\"\"\n\n    num_indices = [np.where(MNIST_labels==x) for x in range(10)]\n    num_data = [MNIST_data[inds] for inds in num_indices]\n    image_mapping = dict(zip(unique_characters[:10], num_data))\n    image_mapping['-'] = generate_images()\n    image_mapping['+'] = generate_images(sign='+')\n    image_mapping['*'] = generate_images(sign='*')\n    image_mapping[' '] = np.zeros([1, 28, 28])\n\n    X_text, X_img, y_text, y_img = [], [], [], []\n\n    for i in range(highest_integer + 1):      # First addend\n        for j in range(highest_integer + 1):  # Second addend\n            for sign in operands: # Create all possible combinations of operands\n                query_string = to_padded_chars(str(i) + sign + str(j), max_len=max_query_length, pad_right=True)\n                query_image = []\n                for n, char in enumerate(query_string):\n                    image_set = image_mapping[char]\n                    index = np.random.randint(0, len(image_set), 1)\n                    query_image.append(image_set[index].squeeze())\n\n                result = eval(query_string)\n                result_string = to_padded_chars(result, max_len=max_answer_length, pad_right=True)\n                result_image = []\n                for n, char in enumerate(result_string):\n                    image_set = image_mapping[char]\n                    index = np.random.randint(0, len(image_set), 1)\n                    result_image.append(image_set[index].squeeze())\n\n                X_text.append(query_string)\n                X_img.append(np.stack(query_image))\n                y_text.append(result_string)\n                y_img.append(np.stack(result_image))\n\n    return np.stack(X_text), np.stack(X_img)/255., np.stack(y_text), np.stack(y_img)/255.\n\ndef to_padded_chars(integer, max_len=3, pad_right=False):\n    \"\"\"\n    Returns a string of len()=max_len, containing the integer padded with ' ' on either right or left side\n    \"\"\"\n    length = len(str(integer))\n    padding = (max_len - length) * ' '\n    if pad_right:\n        return str(integer) + padding\n    else:\n        return padding + str(integer)\n\n\n# %% [markdown] {\"id\":\"bTMHYBf4baih\"}\n# # Creating our data\n# \n# The dataset consists of 20000 samples that (additions and subtractions between all 2-digit integers) and they have two kinds of inputs and label modalities:\n# \n#   **X_text**: strings containing queries of length 5: ['  1+1  ', '11-18', ...]\n# \n#   **X_image**: a stack of images representing a single query, dimensions: [5, 28, 28]\n# \n#   **y_text**: strings containing answers of length 3: ['  2', '156']\n# \n#   **y_image**: a stack of images that represents the answer to a query, dimensions: [3, 28, 28]\n\n# %% [code] {\"id\":\"y0N1vtdLJmkG\",\"outputId\":\"56ee9729-7481-4d56-a86f-8db2265b28b9\",\"execution\":{\"iopub.status.busy\":\"2024-12-05T14:42:52.612035Z\",\"iopub.execute_input\":\"2024-12-05T14:42:52.612281Z\",\"iopub.status.idle\":\"2024-12-05T14:42:57.920761Z\",\"shell.execute_reply.started\":\"2024-12-05T14:42:52.612257Z\",\"shell.execute_reply\":\"2024-12-05T14:42:57.91964Z\"}}\n# Illustrate the generated query/answer pairs\n\nunique_characters = '0123456789+- '       # All unique characters that are used in the queries (13 in total: digits 0-9, 2 operands [+, -], and a space character ' '.)\nhighest_integer = 99                      # Highest value of integers contained in the queries\n\nmax_int_length = len(str(highest_integer))# Maximum number of characters in an integer\nmax_query_length = max_int_length * 2 + 1 # Maximum length of the query string (consists of two integers and an operand [e.g. '22+10'])\nmax_answer_length = 3    # Maximum length of the answer string (the longest resulting query string is ' 1-99'='-98')\n\n# Create the data (might take around a minute)\n(MNIST_data, MNIST_labels), _ = tf.keras.datasets.mnist.load_data()\nX_text, X_img, y_text, y_img = create_data(highest_integer)\nprint(X_text.shape, X_img.shape, y_text.shape, y_img.shape)\n\n\n## Display the samples that were created\ndef display_sample(n):\n    labels = ['X_img:', 'y_img:']\n    for i, data in enumerate([X_img, y_img]):\n        plt.subplot(1,2,i+1)\n        # plt.set_figheight(15)\n        plt.axis('off')\n        plt.title(labels[i])\n        plt.imshow(np.hstack(data[n]), cmap='gray')\n    print('='*50, f'\\nQuery #{n}\\n\\nX_text: \"{X_text[n]}\" = y_text: \"{y_text[n]}\"')\n    plt.show()\n\nfor _ in range(10):\n    display_sample(np.random.randint(0, 10000, 1)[0])\n\n# %% [markdown] {\"id\":\"h06Gi5l63EvS\"}\n# ## Helper functions\n# \n# The functions below will help with input/output of the data.\n\n# %% [code] {\"id\":\"rePVm6duJmkJ\",\"outputId\":\"ac690487-13f9-4745-d31d-1a73905a906e\",\"execution\":{\"iopub.status.busy\":\"2024-12-05T14:42:57.922132Z\",\"iopub.execute_input\":\"2024-12-05T14:42:57.922521Z\",\"iopub.status.idle\":\"2024-12-05T14:42:58.02201Z\",\"shell.execute_reply.started\":\"2024-12-05T14:42:57.922479Z\",\"shell.execute_reply\":\"2024-12-05T14:42:58.020941Z\"}}\n# One-hot encoding/decoding the text queries/answers so that they can be processed using RNNs\n# You should use these functions to convert your strings and read out the output of your networks\n\ndef encode_labels(labels, max_len=3):\n  n = len(labels)\n  length = len(labels[0])\n  char_map = dict(zip(unique_characters, range(len(unique_characters))))\n  one_hot = np.zeros([n, length, len(unique_characters)])\n  for i, label in enumerate(labels):\n      m = np.zeros([length, len(unique_characters)])\n      for j, char in enumerate(label):\n          m[j, char_map[char]] = 1\n      one_hot[i] = m\n\n  return one_hot\n\n\ndef decode_labels(labels):\n    pred = np.argmax(labels, axis=1)\n    predicted = ''.join([unique_characters[i] for i in pred])\n\n    return predicted\n\nX_text_onehot = encode_labels(X_text)\ny_text_onehot = encode_labels(y_text)\n\nprint(X_text_onehot.shape, y_text_onehot.shape)\n\n# %% [markdown] {\"id\":\"7-pNByj-JmkL\"}\n# ---\n# ---\n# \n# ## I. Text-to-text RNN model\n# \n# The following code showcases how Recurrent Neural Networks (RNNs) are built using Keras. Several new layers are going to be used:\n# \n# 1. LSTM\n# 2. TimeDistributed\n# 3. RepeatVector\n# \n# The code cell below explains each of these new components.\n# \n# <img src=\"https://i.ibb.co/NY7FFTc/Screenshot-2023-11-10-at-09-27-25.png\" alt=\"Screenshot-2023-11-10-at-09-27-25\" border=\"0\" width=\"500\"></a>\n# \n\n# %% [code] {\"id\":\"Tll54DIrZwbK\",\"execution\":{\"iopub.status.busy\":\"2024-12-05T14:42:58.023158Z\",\"iopub.execute_input\":\"2024-12-05T14:42:58.023528Z\",\"iopub.status.idle\":\"2024-12-05T14:42:58.030288Z\",\"shell.execute_reply.started\":\"2024-12-05T14:42:58.023499Z\",\"shell.execute_reply\":\"2024-12-05T14:42:58.029205Z\"}}\ndef build_text2text_model(add_more_LSTM=False):\n\n    # We start by initializing a sequential model\n    text2text = tf.keras.Sequential()\n\n    # \"Encode\" the input sequence using an RNN, producing an output of size 256.\n    # In this case the size of our input vectors is [5, 13] as we have queries of length 5 and 13 unique characters. Each of these 5 elements in the query will be fed to the network one by one,\n    # as shown in the image above (except with 5 elements).\n    # Hint: In other applications, where your input sequences have a variable length (e.g. sentences), you would use input_shape=(None, unique_characters).\n    if not add_more_LSTM:\n        text2text.add(LSTM(256, input_shape=(None, len(unique_characters))))\n    else:\n        text2text.add(LSTM(256, input_shape=(None, len(unique_characters)),return_sequences=True))\n        text2text.add(LSTM(256))\n\n    # As the decoder RNN's input, repeatedly provide with the last output of RNN for each time step. Repeat 3 times as that's the maximum length of the output (e.g. '  1-99' = '-98')\n    # when using 2-digit integers in queries. In other words, the RNN will always produce 3 characters as its output.\n    text2text.add(RepeatVector(max_answer_length))\n\n    # By setting return_sequences to True, return not only the last output but all the outputs so far in the form of (num_samples, timesteps, output_dim). This is necessary as TimeDistributed in the below expects\n    # the first dimension to be the timesteps.\n    text2text.add(LSTM(256, return_sequences=True))\n\n    # Apply a dense layer to the every temporal slice of an input. For each of step of the output sequence, decide which character should be chosen.\n    text2text.add(TimeDistributed(Dense(len(unique_characters), activation='softmax')))\n\n    # Next we compile the model using categorical crossentropy as our loss function.\n    text2text.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    text2text.summary()\n\n    return text2text\n\n# %% [code] {\"id\":\"mz6QkPAdzywk\",\"execution\":{\"iopub.status.busy\":\"2024-12-05T14:42:58.031685Z\",\"iopub.execute_input\":\"2024-12-05T14:42:58.032096Z\",\"iopub.status.idle\":\"2024-12-05T14:42:58.046137Z\",\"shell.execute_reply.started\":\"2024-12-05T14:42:58.032057Z\",\"shell.execute_reply\":\"2024-12-05T14:42:58.045103Z\"},\"_kg_hide-input\":true}\n## Your code (look at the assignment description for your tasks for text-to-text model):\ndef train_text_2_text_model(x_train,x_test,y_train,y_test,batch_size=32, epochs=50,patience=15):\n\n    text2text_model = build_text2text_model()\n    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=patience,restore_best_weights=True)\n\n    # Train the model with Early Stopping\n    history = text2text_model.fit(\n        x_train,\n        y_train,\n        batch_size=batch_size,\n        epochs=epochs,\n        validation_data=(x_test,y_test),\n        callbacks=[early_stopping_cb]\n    )\n\n    # Plot the training and validation loss\n    plt.plot(history.history['loss'], label='Training Loss')\n    plt.legend()\n    plt.title('Loss Curve')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.show()\n    return text2text_model\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-05T14:42:58.047606Z\",\"iopub.execute_input\":\"2024-12-05T14:42:58.047938Z\",\"iopub.status.idle\":\"2024-12-05T14:42:58.063484Z\",\"shell.execute_reply.started\":\"2024-12-05T14:42:58.047912Z\",\"shell.execute_reply\":\"2024-12-05T14:42:58.062569Z\"},\"id\":\"GbC6X2Mzbm8Z\"}\n# Helper function to check if borrowing is required\ndef requires_borrowing(query):\n    if \"-\" not in query:\n        return False  # Not subtraction\n    minuend, subtrahend = query.split(\"-\")\n    minuend, subtrahend = minuend.strip(), subtrahend.strip()\n    for min_digit, sub_digit in zip(reversed(minuend), reversed(subtrahend)):\n        if int(min_digit) < int(sub_digit):\n            return True  # Borrowing required\n    return False\n\n# Helper function to check if carrying is required\ndef requires_carry(query):\n    if \"+\" not in query:\n        return False  # Not addition\n    addend1, addend2 = query.split(\"+\")\n    addend1, addend2 = addend1.strip(), addend2.strip()\n    for add_digit1, add_digit2 in zip(reversed(addend1), reversed(addend2)):\n        if int(add_digit1) + int(add_digit2) > 9:\n            return True  # Carrying required\n    return False\n\ndef error_analysis(x_test,y_pre):\n    misclassified = []\n    total_characters = 0\n    correct_characters = 0\n\n    plus_errors = 0\n    minus_errors = 0\n    plus_total = 0\n    minus_total = 0\n\n    borrowing_cases = 0\n    borrowing_errors = 0\n    carry_cases = 0\n    carry_errors = 0\n\n    for i in range(y_pre.shape[0]):\n        predict = decode_labels(y_pre[i])\n        true = decode_labels(y_test[i])\n\n        total_characters += len(true)\n        correct_characters += sum(p == t for p, t in zip(predict, true))\n\n        query = decode_labels(x_test[i])\n\n        if \"+\" in query:\n            plus_total+=1\n            if predict != true:\n                 plus_errors+=1\n\n        if \"-\" in query:\n            minus_total+=1\n            if predict != true:\n                 minus_errors+=1\n\n        if requires_carry(query):\n            carry_cases += 1\n            if predict != true:\n                carry_errors+=1\n\n        if requires_borrowing(query):\n            borrowing_cases += 1\n            if predict != true:\n                borrowing_errors+=1\n\n        if predict != true:\n            misclassified.append((i, predict, true,query))\n\n\n    print(f\"Number of misclassification: {len(misclassified)}, error_ratio:{len(misclassified)/(y_pre.shape[0])*100}%\")\n    print(f\"Character-level accuracy: {correct_characters / total_characters:.2%}\")\n\n    # Calculate error rates\n    plus_error_rate = (plus_errors / plus_total) * 100 if plus_total > 0 else 0\n    minus_error_rate = (minus_errors / minus_total) * 100 if minus_total > 0 else 0\n\n    borrowing_error_rate = (borrowing_errors / borrowing_cases) * 100 if borrowing_cases > 0 else 0\n    carry_error_rate = (carry_errors / carry_cases) * 100 if carry_cases > 0 else 0\n    borrow_carry_ratio = (carry_errors + borrowing_errors)/len(misclassified) * 100 if len(misclassified) > 0 else 0\n\n    # Display results\n    print(f\"Addition (+) Error Rate: {plus_error_rate:.2f}% ({plus_errors}/{plus_total})\")\n    print(f\"Subtraction (-) Error Rate: {minus_error_rate:.2f}% ({minus_errors}/{minus_total})\")\n    print(f\"Borrowing (->) Error Rate: {borrowing_error_rate:.2f}% ({borrowing_errors}/{borrowing_cases})\")\n    print(f\"Carrying (<-) Error Rate: {carry_error_rate:.2f}% ({carry_errors}/{carry_cases})\")\n    print(f\"Borrow_Carry_Error/Total_Error: {borrow_carry_ratio:.2f} % ({carry_errors + borrowing_errors}/{len(misclassified)})\")\n\n    # Visualize\n    for idx, pred, true,query in misclassified[:10]:\n        print(f\"Sample #{idx}\")\n        print(f\"Query: {query}\")\n        print(f\"Predicted: {pred}\")\n        print(f\"True: {true}\")\n        print(\"=\" * 50)\n\n\n# %% [markdown] {\"id\":\"o84zdO-hbm8Z\"}\n# ### Train-Test(50-50)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-05T14:42:58.065917Z\",\"iopub.execute_input\":\"2024-12-05T14:42:58.0662Z\",\"iopub.status.idle\":\"2024-12-05T14:45:03.316015Z\",\"shell.execute_reply.started\":\"2024-12-05T14:42:58.066176Z\",\"shell.execute_reply\":\"2024-12-05T14:45:03.314886Z\"},\"id\":\"ISK7qs0fbm8Z\",\"outputId\":\"4d1c619a-3ce1-44c4-d66b-b02c481fd894\"}\n(x_train,x_test,y_train,y_test)= train_test_split(X_text_onehot,y_text_onehot,test_size=0.5)\nprint(x_train.shape,x_test.shape,y_train.shape,y_test.shape)\n\ntext2text_model1 = train_text_2_text_model(x_train,x_test,y_train,y_test,32,50,15)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-05T14:45:03.317464Z\",\"iopub.execute_input\":\"2024-12-05T14:45:03.317923Z\",\"iopub.status.idle\":\"2024-12-05T14:45:09.605022Z\",\"shell.execute_reply.started\":\"2024-12-05T14:45:03.317876Z\",\"shell.execute_reply\":\"2024-12-05T14:45:09.603469Z\"},\"id\":\"vG5ghC4zbm8Z\",\"outputId\":\"c5869c86-37f3-458f-b6f5-34445e0e7821\"}\nloss, accuracy = text2text_model1.evaluate(x_test,y_test)\ny_pre = text2text_model1(x_test)\nerror_analysis(x_test,y_pre)\n\n# %% [markdown] {\"id\":\"wofpGQxkbm8Z\"}\n# ### Train-Test(25-75)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-05T14:45:09.606927Z\",\"iopub.execute_input\":\"2024-12-05T14:45:09.607407Z\",\"iopub.status.idle\":\"2024-12-05T14:48:17.941223Z\",\"shell.execute_reply.started\":\"2024-12-05T14:45:09.607349Z\",\"shell.execute_reply\":\"2024-12-05T14:48:17.940442Z\"},\"id\":\"E7FTkMwXbm8Z\",\"outputId\":\"539e2bc0-1a20-42a0-f189-fa6385ecfb50\"}\n(x_train,x_test,y_train,y_test)= train_test_split(X_text_onehot,y_text_onehot,test_size=0.75)\nprint(x_train.shape,x_test.shape,y_train.shape,y_test.shape)\n\ntext2text_model2 = train_text_2_text_model(x_train,x_test,y_train,y_test,32,100,15)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-05T14:48:17.942217Z\",\"iopub.execute_input\":\"2024-12-05T14:48:17.942561Z\",\"iopub.status.idle\":\"2024-12-05T14:48:19.106207Z\",\"shell.execute_reply.started\":\"2024-12-05T14:48:17.94252Z\",\"shell.execute_reply\":\"2024-12-05T14:48:19.105564Z\"},\"id\":\"ed7akaEDbm8Z\",\"outputId\":\"45e72a9e-66c8-4b3d-abb8-eef3157cd098\"}\nloss, accuracy = text2text_model2.evaluate(x_test,y_test)\n# y_pre = text2text_model2(x_test)\n# error_analysis(x_test,y_pre)\n\n# %% [markdown] {\"id\":\"eYGsHDYEbm8Z\"}\n# ### Train-Test(10-90)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-05T14:48:19.107443Z\",\"iopub.execute_input\":\"2024-12-05T14:48:19.107821Z\",\"iopub.status.idle\":\"2024-12-05T14:50:59.285799Z\",\"shell.execute_reply.started\":\"2024-12-05T14:48:19.107783Z\",\"shell.execute_reply\":\"2024-12-05T14:50:59.284885Z\"},\"id\":\"4YOs5ir6bm8a\",\"outputId\":\"691fc95b-7a8b-4d53-b6f6-2dad6450228a\"}\n(x_train,x_test,y_train,y_test)= train_test_split(X_text_onehot,y_text_onehot,test_size=0.9)\nprint(x_train.shape,x_test.shape,y_train.shape,y_test.shape)\n\ntext2text_model3 = train_text_2_text_model(x_train,x_test,y_train,y_test,32,100,15)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-05T14:50:59.286818Z\",\"iopub.execute_input\":\"2024-12-05T14:50:59.28708Z\",\"iopub.status.idle\":\"2024-12-05T14:51:00.724556Z\",\"shell.execute_reply.started\":\"2024-12-05T14:50:59.287054Z\",\"shell.execute_reply\":\"2024-12-05T14:51:00.723924Z\"},\"id\":\"iyEq2QUXbm8a\",\"outputId\":\"1d63e61b-6b05-4121-ce20-fc4cbcdaa205\"}\nloss, accuracy = text2text_model3.evaluate(x_test,y_test)\n# y_pre = text2text_model3(x_test)\n# error_analysis(x_test,y_pre)\n\n# %% [markdown] {\"id\":\"MTUqbQrbaKfB\"}\n# \n# ---\n# ---\n# \n# ## II. Image to text RNN Model\n# \n# Hint: There are two ways of building the encoder for such a model - again by using the regular LSTM cells (with flattened images as input vectors) or recurrect convolutional layers [ConvLSTM2D](https://keras.io/api/layers/recurrent_layers/conv_lstm2d/).\n# \n# The goal here is to use **X_img** as inputs and **y_text** as outputs.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-05T14:51:00.725861Z\",\"iopub.execute_input\":\"2024-12-05T14:51:00.726199Z\",\"iopub.status.idle\":\"2024-12-05T14:51:00.932644Z\",\"shell.execute_reply.started\":\"2024-12-05T14:51:00.72616Z\",\"shell.execute_reply\":\"2024-12-05T14:51:00.931853Z\"},\"id\":\"wCy3Gdu1bm8a\",\"outputId\":\"b9774635-7bf6-4948-a12d-9138291f0268\"}\nfrom keras.layers import  MaxPooling2D\n\ndef build_img2text_model(add_more_LSTM=False):\n    model=tf.keras.models.Sequential()\n    model.add(TimeDistributed(Conv2D(16,kernel_size=(5,5), padding='same', activation='relu'), input_shape=(5,28,28,1)))\n    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n    model.add(TimeDistributed(Conv2D(32,kernel_size=(2,2), padding='same',activation='relu')))\n    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n    model.add(TimeDistributed(Conv2D(64,kernel_size=(2,2), padding='same',activation='relu')))\n    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n    model.add(TimeDistributed(Conv2D(128,kernel_size=(2,2), padding='same',activation='relu')))\n\n    model.add(TimeDistributed(Flatten()))\n    model.add(TimeDistributed(Dense(64,activation='linear')))\n    if not add_more_LSTM:\n        model.add(LSTM(256, return_sequences=False))\n    else:\n        model.add(LSTM(256,return_sequences=True))\n        model.add(LSTM(256))\n\n    model.add(RepeatVector(max_answer_length))\n\n    model.add(LSTM(256, return_sequences=True))\n    model.add(TimeDistributed(Dense(len(unique_characters), activation='softmax')))\n\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.summary()\n    return model\n\nimage2text_model = build_img2text_model()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-05T14:51:00.933832Z\",\"iopub.execute_input\":\"2024-12-05T14:51:00.934102Z\",\"iopub.status.idle\":\"2024-12-05T14:57:52.904354Z\",\"shell.execute_reply.started\":\"2024-12-05T14:51:00.934077Z\",\"shell.execute_reply\":\"2024-12-05T14:57:52.903536Z\"},\"id\":\"P0jMSTnTbm8a\",\"outputId\":\"e1da14fa-58e9-4a11-9e13-80ef3e83dc12\"}\nX_img = X_img.reshape((-1, 5, 28, 28, 1))\n(x_img_train,x_img_test,y_text_train,y_text_test)= train_test_split(X_img,y_text_onehot,test_size=0.5)\nprint(x_img_train.shape,x_img_test.shape,y_text_train.shape,y_text_test.shape)\n\n# Build the model\nimage2text_model = build_img2text_model()\n\n# Train the model\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=20,restore_best_weights=True)\nhistory = image2text_model.fit(\n    x_img_train,\n    y_text_train,\n    batch_size=16,\n    epochs=50,\n    validation_data=(x_img_test,y_text_test),\n    callbacks=[early_stopping_cb]\n)\n\n# Plot training/test accuracy\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.legend()\nplt.title('Accuracy Curve')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.show()\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-05T14:57:52.905619Z\",\"iopub.execute_input\":\"2024-12-05T14:57:52.906013Z\",\"iopub.status.idle\":\"2024-12-05T14:57:56.025332Z\",\"shell.execute_reply.started\":\"2024-12-05T14:57:52.905973Z\",\"shell.execute_reply\":\"2024-12-05T14:57:56.024619Z\"},\"id\":\"9lXfT1e3bm8a\",\"outputId\":\"c7e761ca-8422-4516-8627-44273e562bcc\"}\nloss, accuracy = image2text_model.evaluate(x_img_test,y_text_test)\n\n# Predict on a sample\nsample_idx = 1  # Choose a sample\npre= image2text_model.predict(x_img_test[sample_idx:sample_idx+1])\npredicted_text = decode_labels(pre[0])\ntrue_text = decode_labels(y_text_test[sample_idx])\nprint(f\"Predicted: {predicted_text}, True: {true_text}\")\n\n# %% [markdown] {\"id\":\"GnGyo_DIlymz\"}\n# ---\n# ---\n# \n# ## III. Text to image RNN Model\n# \n# Hint: to make this model work really well you could use deconvolutional layers in your decoder (you might need to look up ***Conv2DTranspose*** layer). However, regular vector-based decoder will work as well.\n# \n# The goal here is to use **X_text** as inputs and **y_img** as outputs.\n\n# %% [code] {\"id\":\"KwhT9fiW7p_t\",\"execution\":{\"iopub.status.busy\":\"2024-12-05T14:57:56.026248Z\",\"iopub.execute_input\":\"2024-12-05T14:57:56.026478Z\",\"iopub.status.idle\":\"2024-12-05T14:57:56.030305Z\",\"shell.execute_reply.started\":\"2024-12-05T14:57:56.026455Z\",\"shell.execute_reply\":\"2024-12-05T14:57:56.029491Z\"},\"outputId\":\"5f17cd6c-b903-4669-b578-f737923b3590\"}\nfrom tensorflow import keras\nfrom functools import partial\n\n# Model parameters\ntext_dim = 13  # Number of unique characters in queries (13)\nmax_query_len = max_query_length  # Length of input queries (5)\nimage_shape = (max_answer_length, 28, 28)  # Shape of output image sequence\nlstm_units = 512\n\ndense = partial(keras.layers.Dense,7 * 7 * 64,activation='relu')\nreshape = partial(keras.layers.Reshape,(7, 7, 64))\nconvT1 = partial(keras.layers.Conv2DTranspose,32, (3, 3), activation='relu', strides=(2, 2), padding=\"same\")\nconvT2 = partial(keras.layers.Conv2DTranspose,1, (3, 3), activation=\"sigmoid\", strides=(2, 2), padding=\"same\")\nlstm1 = partial(keras.layers.LSTM,return_sequences=False)\nlstm2 = partial(keras.layers.LSTM,return_sequences=True)\nrepeat_vector = partial(keras.layers.RepeatVector,image_shape[0])\ninp = partial(keras.layers.Input,shape=(max_query_len, text_dim))\n\ntext_to_image_rnn = keras.models.Sequential([\ninp(),\nlstm1(lstm_units),\nrepeat_vector(),\nlstm2(lstm_units),\nTimeDistributed(dense()),\nTimeDistributed(reshape()),\nTimeDistributed(convT1()),\nTimeDistributed(convT2())\n])\n\n# Instantiate and compile the model\ntext_to_image_rnn.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])\ntext_to_image_rnn.summary()\n\n# %% [code] {\"id\":\"xgEzPfyj1lgF\"}\nX_text_onehot = encode_labels(X_text, max_len=max_query_length)\n\ndef decay_rate(epoch,lr):\n  decay = 1e-1\n  return lr/(1+decay*epoch)\n\nearly_stopping=keras.callbacks.EarlyStopping(monitor='val_loss',patience=10,mode='min',restore_best_weights=True)\ndecay_lr=keras.callbacks.LearningRateScheduler(decay_rate)\n\nhistory = text_to_image_rnn.fit(\n    X_text_onehot, y_img,\n    validation_split=0.1,\n    epochs=30,\n    batch_size=16\n)\n\n# %% [code] {\"id\":\"zexxWPWdl6pM\"}\npred_img = text_to_image_rnn.predict(X_text_onehot[128:138])\n\nfor i in range(10):\n    plt.figure(figsize=(10, 5))\n    plt.suptitle(f\"Query: {X_text[128+i]}\")\n    for t in range(max_answer_length):\n        plt.subplot(1, max_answer_length, t + 1)\n        plt.axis('off')\n        plt.imshow(pred_img[i][t], cmap=\"gray\")\n    plt.show()\n\n# %% [markdown] {\"id\":\"UsRnLWPTbm8a\"}\n# ## IV. Adding additional LSTM layers\n\n# %% [markdown] {\"id\":\"6r-pRM_tcsM2\"}\n# # Text-Text\n\n# %% [code] {\"id\":\"0qM4tf2Hc8y4\"}\n## Your code (look at the assignment description for your tasks for text-to-text model):\ndef train_text_2_text_model(x_train,x_test,y_train,y_test,batch_size=32, epochs=50,patience=15):\n\n    text2text_model = build_text2text_model(add_more_LSTM=True)\n    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=patience,restore_best_weights=True)\n\n    # Train the model with Early Stopping\n    history = text2text_model.fit(\n        x_train,\n        y_train,\n        batch_size=batch_size,\n        epochs=epochs,\n        validation_data=(x_test,y_test),\n        callbacks=[early_stopping_cb]\n    )\n\n    # Plot the training and validation loss\n    plt.plot(history.history['loss'], label='Training Loss')\n    plt.legend()\n    plt.title('Loss Curve')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.show()\n    return text2text_model\n\n# %% [code] {\"id\":\"LS6l0ZxeecfI\"}\n(x_train,x_test,y_train,y_test)= train_test_split(X_text_onehot,y_text_onehot,test_size=0.5)\nprint(x_train.shape,x_test.shape,y_train.shape,y_test.shape)\n\ntext2text_model1 = train_text_2_text_model(x_train,x_test,y_train,y_test,32,50,15)\n\n# %% [code] {\"id\":\"B6Jp5RrXenWl\"}\nloss, accuracy = text2text_model1.evaluate(x_test,y_test)\ny_pre = text2text_model1(x_test)\nerror_analysis(x_test,y_pre)\n\n# %% [markdown] {\"id\":\"fbIshXpQc5W4\"}\n# # Image-Text\n\n# %% [code] {\"id\":\"1YeHhBygdGPP\"}\nX_img = X_img.reshape((-1, 5, 28, 28, 1))\n(x_img_train,x_img_test,y_text_train,y_text_test)= train_test_split(X_img,y_text_onehot,test_size=0.5)\nprint(x_img_train.shape,x_img_test.shape,y_text_train.shape,y_text_test.shape)\n\n# Build the model\nimage2text_model = build_img2text_model(add_more_LSTM=True)\n\n# Train the model\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=20,restore_best_weights=True)\nhistory = image2text_model.fit(\n    x_img_train,\n    y_text_train,\n    batch_size=16,\n    epochs=50,\n    validation_data=(x_img_test,y_text_test),\n    callbacks=[early_stopping_cb]\n)\n\n# Plot training/test accuracy\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.legend()\nplt.title('Accuracy Curve')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.show()\n\n\n# %% [code] {\"id\":\"Kr0U_MG7dHAL\"}\nloss, accuracy = image2text_model.evaluate(x_img_test,y_text_test)\n\n# Predict on a sample\nsample_idx = 1  # Choose a sample\npre= image2text_model.predict(x_img_test[sample_idx:sample_idx+1])\npredicted_text = decode_labels(pre[0])\ntrue_text = decode_labels(y_text_test[sample_idx])\nprint(f\"Predicted: {predicted_text}, True: {true_text}\")\n\n# %% [markdown] {\"id\":\"9ofdK9VMdJbb\"}\n# # Image-Image\n\n# %% [code] {\"id\":\"Pib9w3eXdNly\"}\nfrom tensorflow import keras\nfrom functools import partial\n\n# Model parameters\ntext_dim = 13  # Number of unique characters in queries (13)\nmax_query_len = max_query_length  # Length of input queries (5)\nimage_shape = (max_answer_length, 28, 28)  # Shape of output image sequence\nlstm_units = 512\n\ndense = partial(keras.layers.Dense,7 * 7 * 64,activation='relu')\nreshape = partial(keras.layers.Reshape,(7, 7, 64))\nconvT1 = partial(keras.layers.Conv2DTranspose,32, (3, 3), activation='relu', strides=(2, 2), padding=\"same\")\nconvT2 = partial(keras.layers.Conv2DTranspose,1, (3, 3), activation=\"sigmoid\", strides=(2, 2), padding=\"same\")\nlstm1 = partial(keras.layers.LSTM,return_sequences=False)\nlstm2 = partial(keras.layers.LSTM,return_sequences=True)\nrepeat_vector = partial(keras.layers.RepeatVector,image_shape[0])\ninp = partial(keras.layers.Input,shape=(max_query_len, text_dim))\n\ntext_to_image_rnn = keras.models.Sequential([\ninp(),\nlstm2(lstm_units),\nlstm1(lstm_units),\nrepeat_vector(),\nlstm2(lstm_units),\nTimeDistributed(dense()),\nTimeDistributed(reshape()),\nTimeDistributed(convT1()),\nTimeDistributed(convT2())\n])\n\n# Instantiate and compile the model\ntext_to_image_rnn.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])\ntext_to_image_rnn.summary()\n\n# %% [code] {\"id\":\"YWQ1-4fudWm4\"}\nX_text_onehot = encode_labels(X_text, max_len=max_query_length)\n\ndef decay_rate(epoch,lr):\n  decay = 1e-1\n  return lr/(1+decay*epoch)\n\nearly_stopping=keras.callbacks.EarlyStopping(monitor='val_loss',patience=10,mode='min',restore_best_weights=True)\ndecay_lr=keras.callbacks.LearningRateScheduler(decay_rate)\n\nhistory = text_to_image_rnn.fit(\n    X_text_onehot, y_img,\n    validation_split=0.1,\n    epochs=30,\n    batch_size=16\n)\n\n# %% [code] {\"id\":\"cOJdXGXtddIV\"}\npred_img = text_to_image_rnn.predict(X_text_onehot[128:138])\n\nfor i in range(10):\n    plt.figure(figsize=(10, 5))\n    plt.suptitle(f\"Query: {X_text[128+i]}\")\n    for t in range(max_answer_length):\n        plt.subplot(1, max_answer_length, t + 1)\n        plt.axis('off')\n        plt.imshow(pred_img[i][t], cmap=\"gray\")\n    plt.show()","metadata":{"_uuid":"ed368436-fcac-471d-81e2-759554ca568b","_cell_guid":"61a6c747-c37f-4d5c-869c-ce353d8a905c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}