# -*- coding: utf-8 -*-
"""Textmining_Task2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mrY11Huv7vxHoaYXacxzySa_rdZlSmtA
"""

import warnings
warnings.filterwarnings("ignore")
!pip install transformers
!pip install datasets


## Data Conversion


from transformers import AutoTokenizer

# Load a pretrained tokenizer
model_checkpoint = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

# Lets check if it comes with a fast version
tokenizer.is_fast

# Function to read IOB format data
def read_iob_file(file_path):
    with open(file_path, "r") as f:
        sentences = []
        tokens = []
        labels = []

        for line in f:
            if line.strip():  # If the line is not empty
                parts = line.strip().split()

                # Ensure we have at least two items (word and label)
                if len(parts) >= 2:
                    word, label = parts[0], parts[1]
                    tokens.append(word)
                    labels.append(label)
            else:
                # Add the sentence to sentences list and reset tokens and labels for the next sentence
                if tokens:
                    sentences.append((tokens, labels))
                    tokens = []
                    labels = []

        # Add any remaining tokens and labels as the last sentence
        if tokens:
            sentences.append((tokens, labels))
    return sentences

def tokenize_and_align_labels(sentences):
    tokenized_inputs = tokenizer(
        [sentence[0] for sentence in sentences],
        truncation=True,
        is_split_into_words=True,
        padding=True
    )
    aligned_labels = []
    for i, label in enumerate([sentence[1] for sentence in sentences]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        label_ids = []
        previous_word_id = None
        for word_id in word_ids:
            if word_id is None:
                label_ids.append(-100)
            elif word_id != previous_word_id:
                label_ids.append(label[word_id])
            else:
                label_ids.append(label[word_id].replace("B-", "I-"))
            previous_word_id = word_id
        aligned_labels.append(label_ids)
    tokenized_inputs["labels"] = aligned_labels
    return tokenized_inputs
# %% [code]
# Paths to the IOB files
train_data = read_iob_file("/content/train.txt")
val_data = read_iob_file("/content/val.txt")
test_data = read_iob_file("/content/test.txt")

# Tokenize and align labels
train_tokenized = tokenize_and_align_labels(train_data)
val_tokenized = tokenize_and_align_labels(val_data)
test_tokenized = tokenize_and_align_labels(test_data)


# %% [code]
def convert_iob_to_hf_format(data):
    """Converts IOB data to Hugging Face token classification format."""
    tokenized_inputs = []
    labels = []

    for tokens, iob_labels in data:
        # Tokenize the words and align labels with the tokens
        encoding = tokenizer(tokens, truncation=True, padding="max_length", max_length=128, is_split_into_words=True)

        # Extract the word IDs and labels for each word-token pair
        word_ids = encoding.word_ids()
        label_ids = [-100] * len(encoding.input_ids)  # -100 is used to ignore padding tokens

        # Assign labels to the non-padding tokens
        for i, word_id in enumerate(word_ids):
            if word_id is not None:
                label_ids[i] = iob_labels[word_id]  # Assign label for this word

        tokenized_inputs.append(encoding)
        labels.append(label_ids)

    return tokenized_inputs, labels

# Convert IOB data to Hugging Face format
train_tokenized, train_labels = convert_iob_to_hf_format(train_data)
val_tokenized, val_labels = convert_iob_to_hf_format(val_data)
test_tokenized, test_labels = convert_iob_to_hf_format(test_data)

# %% [code]
# Create label2id and id2label mappings
label_names = list(set([label for sublist in train_labels for label in sublist]))  # Get all unique labels
label2id = {label: i for i, label in enumerate(label_names)}
id2label = {i: label for label, i in label2id.items()}

print(f"Label to ID mapping: {label2id}")
# Convert the labels in the train, val, and test sets
def convert_labels_to_ids(labels, label2id):
    return [[label2id.get(label, -100) for label in sublist] for sublist in labels]

# Convert the labels for each dataset
train_labels_int = convert_labels_to_ids(train_labels, label2id)
val_labels_int = convert_labels_to_ids(val_labels, label2id)
test_labels_int = convert_labels_to_ids(test_labels, label2id)
from datasets import Dataset

# Prepare dataset
train_dataset = Dataset.from_dict({
    'input_ids': [x['input_ids'] for x in train_tokenized],
    'attention_mask': [x['attention_mask'] for x in train_tokenized],
    'labels': train_labels_int  # Use the integer labels here
})

val_dataset = Dataset.from_dict({
    'input_ids': [x['input_ids'] for x in val_tokenized],
    'attention_mask': [x['attention_mask'] for x in val_tokenized],
    'labels': val_labels_int  # Use the integer labels here
})

test_dataset = Dataset.from_dict({
    'input_ids': [x['input_ids'] for x in test_tokenized],
    'attention_mask': [x['attention_mask'] for x in test_tokenized],
    'labels': test_labels_int  # Use the integer labels here
})

# %% [code]
from transformers import AutoModelForTokenClassification, Trainer, TrainingArguments

# Load the pretrained model
# Load the pretrained smaller model for faster training (DistilBERT)
model_checkpoint = "distilbert-base-uncased"

model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    num_labels=len(label_names))

# Set up training arguments
training_args = TrainingArguments(
    output_dir="./results",          # output directory
    num_train_epochs=0.1,              # number of training epochs
    per_device_train_batch_size=4,   # batch size for training
    per_device_eval_batch_size=4,    # batch size for evaluation
    warmup_steps=0,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    evaluation_strategy="epoch",     # evaluate every epoch
    save_strategy="epoch",           # save the model every epoch
    load_best_model_at_end=True,     # load the best model when finished training
    logging_steps=10
)

# Initialize Trainer
trainer = Trainer(
    model=model,                       # the pre-trained model
    args=training_args,                # training arguments
    train_dataset=train_dataset,       # training dataset
    eval_dataset=val_dataset,          # evaluation dataset
)

# Train the model
trainer.train()

### Evaluate the Model
# %% [code]
results = trainer.evaluate(test_dataset)
print("Test Results:", results)


from transformers import TrainingArguments, Trainer, AdamW
import numpy as np

# Define hyperparameter grid
learning_rates = [1e-5, 3e-5, 5e-5]
batch_sizes = [8, 16]
weight_decays = [0.0, 0.01]

# Set up variable to track the best configuration
best_config = None
best_eval_metric = float('inf')  # Lower is better for loss

# Iterate over all hyperparameter combinations
for lr in learning_rates:
    for batch_size in batch_sizes:
        for wd in weight_decays:
            print(f"Training with lr={lr}, batch_size={batch_size}, weight_decay={wd}")

            # Set training arguments
            training_args = TrainingArguments(
                output_dir="./results",
                evaluation_strategy="epoch",
                per_device_train_batch_size=batch_size,
                per_device_eval_batch_size=batch_size,
                learning_rate=lr,
                weight_decay=wd,
                num_train_epochs=0.1,  # Number of epochs (default can be modified)
                logging_dir="./logs",
            )

            # Initialize Trainer with the model, training arguments, datasets, and optimizer
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=val_dataset,
                optimizers=(AdamW(model.parameters(), lr=lr, weight_decay=wd), None),
            )

            # Fine-tune the model and evaluate on validation set
            trainer.train()
            eval_metrics = trainer.evaluate(val_dataset)
            eval_loss = eval_metrics["eval_loss"]

            # Check if this is the best configuration so far
            if eval_loss < best_eval_metric:
                best_eval_metric = eval_loss
                best_config = {
                    "learning_rate": lr,
                    "batch_size": batch_size,
                    "weight_decay": wd
                }

# Output the best configuration
print(f"Best configuration: {best_config} with validation loss: {best_eval_metric}")

# Re-train the model on the best configuration and evaluate on the test set
training_args = TrainingArguments(
    output_dir="./final_model",
    evaluation_strategy="epoch",
    per_device_train_batch_size=best_config["batch_size"],
    per_device_eval_batch_size=best_config["batch_size"],
    learning_rate=best_config["learning_rate"],
    weight_decay=best_config["weight_decay"],
    num_train_epochs=3,
    logging_dir="./final_logs",
)

# Set up the final Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,  # Evaluate on test set for final result
    optimizers=(AdamW(model.parameters(), lr=best_config["learning_rate"], weight_decay=best_config["weight_decay"]), None),
)

# Train and evaluate on test set
trainer.train()
test_metrics = trainer.evaluate(test_dataset)
print(f"Test metrics with best hyperparameters: {test_metrics}")

from sklearn.metrics import classification_report

label_list = [id2label[i] for i in range(len(id2label))]

# Custom evaluation function to compute precision, recall, and F1-score for each entity type
def compute_metrics(predictions):
    # Unpack predictions and labels from the trainer's output
    predictions, labels = predictions
    # Take the argmax to get the predicted class labels
    predictions = predictions.argmax(-1)

    # Filter out unwanted label IDs (e.g., -100) and convert IDs to label names
    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    # Flatten the lists of lists for compatibility with classification_report
    flat_true_labels = [label for sublist in true_labels for label in sublist]
    flat_true_predictions = [label for sublist in true_predictions for label in sublist]

    # Use classification_report to get detailed metrics
    report = classification_report(flat_true_labels, flat_true_predictions, output_dict=True)

    # Extract and return overall metrics as scalars
    overall_precision = report["weighted avg"]["precision"]
    overall_recall = report["weighted avg"]["recall"]
    overall_f1 = report["weighted avg"]["f1-score"]

    return {
        "precision": overall_precision,
        "recall": overall_recall,
        "f1-score": overall_f1,
        "detailed_report": report  # To inspect if needed
    }

# Integrate the custom compute_metrics function into Trainer
trainer.compute_metrics = compute_metrics

# Evaluate on the test set and print the results
results = trainer.evaluate(test_dataset)
print(results)

from sklearn.metrics import f1_score

def compute_average_f1(true_labels, predictions):
    # Flatten lists
    all_true_labels = [item for sublist in true_labels for item in sublist]
    all_predictions = [item for sublist in predictions for item in sublist]

    # Calculate F1 scores
    macro_f1 = f1_score(all_true_labels, all_predictions, average='macro')
    micro_f1 = f1_score(all_true_labels, all_predictions, average='micro')

    return {"macro_f1": macro_f1, "micro_f1": micro_f1}

# Apply to test set predictions
predictions = trainer.predict(test_dataset)
average_f1 = compute_average_f1(predictions.label_ids, predictions.predictions.argmax(-1))
print(average_f1)